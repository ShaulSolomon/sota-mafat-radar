{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "MAFAT_Radar_Challenge_Baseline_ErrorAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r3Rx56zVdb3T",
        "wrLzeJJroRtn"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp9FJgl5VfPx",
        "colab_type": "text"
      },
      "source": [
        "#**MAFAT Radar Challenge - Baseline Model**\n",
        "\n",
        "In this notebook, a Convolutional Neural Network (CNN) baseline model is presented. \n",
        "\n",
        "The model is trained on the training and auxiliary datasets.   \n",
        "In the end, a submission file with predictions for the public test set is being created and downloaded.\n",
        "\n",
        "The raw input for the model is the segments I/Q matrices.   \n",
        "Note that this is a simplistic baseline model. The model should only be used as a boilerplate code to boost development and to accelerate participants' orientation phase. Participants are encouraged to explore different, more creative, approaches, such as data augmentation, unsupervised pre-training/autoencoders, RNNs/transformers, signal processing, feature engineering, transfer learning, etc.   \n",
        "   \n",
        "To learn more about signals, I/Q, Doppler and other terms please go to [Resources](https://competitions.codalab.org/competitions/25389#learn_the_details-resources) on the competition website.    \n",
        "Please read the [Descriptive Statistics notebook](https://colab.research.google.com/drive/11Lzihg2vKIbo4KAIIJxW5CRZIncoWgtL?usp=sharing) to get familiar with the data.\n",
        "   \n",
        "Submissions are evaluated on the area under the Receiver Operating Characteristic Curve ([ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)).   \n",
        "**The baseline model final result on the public test set is:   \n",
        " 0.73 ROC AUC.**\n",
        "\n",
        "[Competition website](https://competitions.codalab.org/competitions/25389)   \n",
        "[MAFAT Challenge homepage](https://mafatchallenge.mod.gov.il/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AaZtlvVZ-2T",
        "colab_type": "text"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTKVzcpr55s7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "bab4f9cc-98bd-4073-d2a3-d7dd13889ca3"
      },
      "source": [
        "!pip install tensorflow-determinism\n",
        "!pip install --upgrade wandb\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-determinism in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.6/dist-packages (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.16.3)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.7)\n",
            "Requirement already satisfied, skipping upgrade: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.3)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied, skipping upgrade: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
            "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4CQDksDbfl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11c9a829-d996-4507-a0f3-2e8b404c046a"
      },
      "source": [
        "from google.colab import drive\n",
        "mount_path = '/content/gdrive/'\n",
        "drive.mount(mount_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSQt35ZLR8Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if os.path.isfile('path'):\n",
        "  root_path = open('path', 'r').read()\n",
        "else:\n",
        "  root_path = input()\n",
        "  path_file = open(\"path\", \"w\")\n",
        "  path_file.write(root_path)\n",
        "  path_file.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_6bG45jUx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37a1def1-5e29-468b-bc24-9cc124b6ecbd"
      },
      "source": [
        "cd {root_path}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WORK/ML/develop/MAFAT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2KNV617UlCX",
        "colab_type": "text"
      },
      "source": [
        "## How to connect to github using ssh\n",
        "\n",
        "[https://medium.com/@ashkanpakzad/data-into-google-colab-5ddeb4f4e8](https://medium.com/@ashkanpakzad/data-into-google-colab-5ddeb4f4e8)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Following commands should be executed only once for setup in order to connect to private github repo:\n",
        "\n",
        "```\n",
        "# to create a private+public keys run the command:\n",
        "!ssh-keygen -t rsa -b 4096 -C “hershkoy@github.com”\n",
        "\n",
        "#this is the private key. copy paste and *SAVE IT* on your local disk for any future use. give it a meaningful name so that you will remember what it is for :)\n",
        "!cat /root/.ssh/id_rsa\n",
        "\n",
        "#this is your public key. copy-paste and upload to github (settings => SSH and GPG keys => New key)\n",
        "!cat /root/.ssh/id_rsa.pub\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShFle-j88JZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "if [ -f \"/root/.ssh/id_rsa\" ]; then\n",
        "    echo \"github key exists.\"\n",
        "else\n",
        "    mkdir /root/.ssh/\n",
        "    cp key.pem /root/.ssh/id_rsa\n",
        "    ssh-keyscan github.com >> /root/.ssh/known_hosts\n",
        "    chmod 644 /root/.ssh/known_hosts\n",
        "    chmod 600 /root/.ssh/id_rsa\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YKlOsLDMy05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf sota-mafat-radar\n",
        "!ssh -T git@github.com\n",
        "!git clone git@github.com:ShaulSolomon/sota-mafat-radar.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSMa29SfVfPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e26df6de-04a2-4fe4-e79c-19c114c183a0"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from termcolor import colored\n",
        "\n",
        "sys.path.insert(0,os.path.join(os.getcwd(),\"sota-mafat-radar/code/utils\"))\n",
        "import experiment_utils as utils\n",
        "\n",
        "\n",
        "# Set seed for reproducibility of results\n",
        "seed_value = 0\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "tf.compat.v1.set_random_seed(seed_value)\n",
        "\n",
        "# Configure a new global `tensorflow` session\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "tf.__version__\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nea1CsBUHW9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WANDB_enable = False\n",
        "\n",
        "if WANDB_enable:\n",
        "  !wandb login {utils.config_parser['DEFAULT'][\"WANDB_LOGIN\"]}\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ZPU_OAXlFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set and test path to competition data files\n",
        "competition_path = os.path.join(os.getcwd(),\"data\") \n",
        "try:\n",
        "  file_path = 'MAFAT RADAR Challenge - Training Set V1.csv'\n",
        "  with open(f'{competition_path}/{file_path}') as f:\n",
        "    f.readlines()\n",
        "  print(colored('Everything is setup correctly', color='green'))\n",
        "except:\n",
        "  print(colored('Please mount drive and set competition_path correctly',\n",
        "                color='red'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1xzWMMFVfP2",
        "colab_type": "text"
      },
      "source": [
        "## **Data Preprocessing**\n",
        "**The preprocessing main steps:**   \n",
        "1. Applying [DFT](https://en.wikipedia.org/wiki/Discrete_Fourier_transform) (Discrete Fourier transform) by using the fast Fourier transform algorithm ([FFT](https://en.wikipedia.org/wiki/Fast_Fourier_transform)) and [Hann function](https://www.mathworks.com/help/signal/ref/hann.html) to smooth the I/Q matrix. Afterward, calculating the absolute value of the I/Q matrix complex numbers.   \n",
        "More information about Windowing and Hann function:   \n",
        "[Wikipedia Hann](https://en.wikipedia.org/wiki/Hann_function#:~:text=The%20Hann%20function%20of%20length,can%20be%20even%20or%20odd.)   \n",
        "[Wikipedia Window function](https://en.wikipedia.org/wiki/Window_function)   \n",
        "2. Set max value on the center of the target's mass.   \n",
        "The doppler burst is a vector (1 x 32) that marks the center of the target's mass on each I/Q matrix in every time unit (32 time-units).   \n",
        "The preprocess sets the maximum value of each I/Q matrix in-place of the values at the center of the target's mass.\n",
        "3. Normalization - Transforming I/Q matrix values to standardized values.   \n",
        "Subtraction of the mean and division by the standard deviation.\n",
        "\n",
        "**Preprocessing main steps explained:**   \n",
        "The DFT converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa.   \n",
        "The I/Q matrix in this challenge is being converted from the velocity domain to the frequency domain.\n",
        "Windows (like \"Hann\") are used to reduce spectral leakage when performing a Fourier Transform on time data and converting it into the frequency domain.\n",
        "\n",
        "The purpose of switching the values of the center of the target's mass in the I/Q matrix with the max value of the matrix is to enhance this part of the matrix and focus the model on this part.\n",
        "\n",
        "\n",
        "Finally, the normalization is used for scaling all the I/Q matrices.\n",
        "\n",
        "* *Note: the target_type lables are strings (\"human\" and \"animal\"), the data_preprocess function replaces them to \"human\" = 1 and \"animal\" = 0.*   \n",
        "   \n",
        "\n",
        "\n",
        "**The FFT Process Explained:**   \n",
        "The IQ matrix contains 32 x 128 elements being 128 I/Q time data samples within a single radar processing time frame and 32 consecutive such time units. The X-axis represents the pulse transmission time, also known as “slow-time”. The Y-axis represents the reception time of signals with respect to pulse transmission time divided into 128 equal sized bins, also known as “fast-time”. The reason FFT is performed on the ‘fast time’ axis (i.e. 128 samples) rather than on the ‘slow time’ axis (i.e. 32 samples) is a matter of scale. An underlying assumption for an effective FFT analysis is that ‘acceleration’ during the time frame represented by the data is very low. Given the type of motion presented by humans and animals only the ‘fast time’ axis complies with this assumption.\n",
        "Therefore, FFT processing should be applied over the dimension of 128 I/Q time samples to get the frequency content of the returned radar signal at each time frame. A spectrogram (as shown in the samples) is generated by attaching together 32 consecutive FFT results (please note that besides FFT processing some additional manipulation is required to properly display a spectrogram – like taking a LOG of the absolute value and passing a threshold – this is well detailed in the ‘Processing’ section of the competition notebook). Therefore, a spectrogram would eventually provide a display of how the frequency content of the returned signal is changing along time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6KvZouVfP6",
        "colab_type": "text"
      },
      "source": [
        "### **Splitting the Training set**\n",
        "\n",
        "The functions below split the training set into Train and Validation sets.\n",
        "\n",
        "* Validation Set.   \n",
        "The validation set is constructed from segments from geolocation_ids 1 and 4.   \n",
        "These geolocations' target_type distributions are balanced.\n",
        "To create a fair representation of the test set   \n",
        "(one segment form each track) every sixth segment was taken to the validation set.   \n",
        "The reason is that approximately 75% of the tracks have less than 6 segments (see the [Descriptive Statistics notebook](https://colab.research.google.com/drive/11Lzihg2vKIbo4KAIIJxW5CRZIncoWgtL?usp=sharing)),   \n",
        "it guarantees that most of the segments in the validation set are not from the same track.   \n",
        "   \n",
        "* Adding \"human\" segments to the training set.   \n",
        "The training set is imbalanced (more animals than humans, see the [Descriptive Statistics notebook](https://colab.research.google.com/drive/11Lzihg2vKIbo4KAIIJxW5CRZIncoWgtL?usp=sharing).   \n",
        "One can address this issue by taking segements of humans from the \"Auxiliary Experiment Set\".   \n",
        "The first 3 segments from each track are taken into the training set (or less if the track is shorter than 3 segments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "170f5Pcgk5gF",
        "colab_type": "text"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrGpOOCGgVdz",
        "colab_type": "text"
      },
      "source": [
        "### **CNN Model**\n",
        "The model has two convolutional layers, both followed by max-pooling layers.    \n",
        "Those layers are followed by 2 fully-connected (dense) layers, activated with a ReLU function and regularized with   \n",
        "L2 regularization, followed by a final output layer with a single neuron with a Sigmoid activation function,   \n",
        "used for final binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm-o5mr6gT7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the model\n",
        "def create_model(input_shape, init):\n",
        "  \"\"\"\n",
        "  CNN model.\n",
        "\n",
        "  Arguments:\n",
        "    input_shape -- the shape of our input\n",
        "    init -- the weight initialization\n",
        "\n",
        "  Returns:\n",
        "    CNN model    \n",
        "  \"\"\"\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer = init, bias_regularizer='l2', input_shape=input_shape))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer = init, bias_regularizer='l2'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, kernel_regularizer = 'l2', activation='relu', kernel_initializer = init))\n",
        "  model.add(Dense(32, kernel_regularizer = 'l2', activation='relu', kernel_initializer = init))\n",
        "  model.add(Dense(1, activation='sigmoid', kernel_initializer = init))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6VRmYQ6VfP_",
        "colab_type": "text"
      },
      "source": [
        "### **Evaluation and Visualization of Model's results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByTli3H1VfQD",
        "colab_type": "text"
      },
      "source": [
        "## **Training The Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKvhQs_9gpVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading and preparing the data\n",
        "\n",
        "# Loading Auxiliary Experiment set - can take a few minutes\n",
        "experiment_auxiliary = 'MAFAT RADAR Challenge - Auxiliary Experiment Set V2'\n",
        "experiment_auxiliary_df = utils.load_data(experiment_auxiliary, folder=competition_path )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMDQ4YgSN_fW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taking sample from the Auxiliary Experiment set\n",
        "train_aux = utils.aux_split(experiment_auxiliary_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcsTx2T_2Uzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The function append_dict is for concatenating the training set \n",
        "# with the Auxiliary data set segments\n",
        "\n",
        "def append_dict(dict1, dict2):\n",
        "  for key in dict1:\n",
        "    dict1[key] = np.concatenate([dict1[key], dict2[key]], axis=0)\n",
        "  return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqdUzjrN-U6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training set\n",
        "train_path = 'MAFAT RADAR Challenge - Training Set V1'\n",
        "training_df = utils.load_data(train_path, folder=competition_path )\n",
        "\n",
        "# Adding segments from the experiment auxiliary set to the training set\n",
        "train_df = append_dict(training_df, train_aux)\n",
        "\n",
        "# Preprocessing and split the data to training and validation\n",
        "train_df = utils.data_preprocess(train_df.copy())\n",
        "train_x, train_y, val_x, val_y, val_idx = utils.split_train_val(train_df)\n",
        "\n",
        "train_df_t = train_df.copy()\n",
        "del train_df_t['doppler_burst']\n",
        "del train_df_t['iq_sweep_burst']\n",
        "train_dff = pd.DataFrame(train_df_t)\n",
        "train_dff['is_validation']=val_idx\n",
        "\n",
        "val_y =  val_y.astype(int)\n",
        "train_y =train_y.astype(int)\n",
        "train_x = train_x.reshape(list(train_x.shape)+[1])\n",
        "val_x = val_x.reshape(list(val_x.shape)+[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3GDCYAZ725I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dff\n",
        "#train_dff.to_csv(\"train_dff.csv\", sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZk7ZKOGb6tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Public test set - loading and preprocessing\n",
        "test_path = 'MAFAT RADAR Challenge - Public Test Set V1'\n",
        "test_df = utils.load_data(test_path, folder=competition_path )\n",
        "test_df = utils.data_preprocess(test_df.copy())\n",
        "test_x = test_df['iq_sweep_burst']\n",
        "test_x = test_x.reshape(list(test_x.shape)+[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfVig_oBa63z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model configuration:\n",
        "batch_size = 16\n",
        "img_width, img_height = 126, 32\n",
        "loss_function = BinaryCrossentropy()\n",
        "no_epochs = 10\n",
        "optimizer = Adam(learning_rate = 0.001)\n",
        "input_shape = (img_width, img_height, 1)\n",
        "\n",
        "init = tf.keras.initializers.GlorotNormal(seed = 0)\n",
        "\n",
        "# Creating and running the model\n",
        "model = create_model(input_shape, init)  \n",
        "model.compile(loss=loss_function, optimizer=optimizer, metrics=[AUC(), 'accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmISWmSlPXtK",
        "colab_type": "text"
      },
      "source": [
        "**Model Architecture**   \n",
        "   \n",
        "![](https://drive.google.com/uc?export=view&id=1wsJBHbghEPGT0s1QQG6BHl7MS3Yo0o4i)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZVukitT3-cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PXlRHub7ae-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model fit\n",
        "\n",
        "wandb.init(project=\"mafat\",name=\"first\")\n",
        "\n",
        "callbacks = []\n",
        "if WANDB_enable:\n",
        "  callbacks.append(WandbCallback())\n",
        "\n",
        "history = model.fit(train_x, train_y, batch_size = batch_size, epochs = no_epochs, \n",
        "                    validation_data = (val_x, val_y), callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Rx56zVdb3T",
        "colab_type": "text"
      },
      "source": [
        "#### **Results**\n",
        "Submissions are evaluated on the area under the Receiver Operating Characteristic Curve ([ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic))   \n",
        "on the predicted probabilities, as calculated by [roc_auc_score in scikit-learn (v 0.23.1)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RCwJ8CWoLyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot ROC curve and show ROC-AUC results of the training and validation sets. \n",
        "pred = [model.predict(train_x), model.predict(val_x)]\n",
        "actual = [train_y, val_y]\n",
        "utils.stats(pred, actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrLzeJJroRtn",
        "colab_type": "text"
      },
      "source": [
        "## **Final Submission File**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn6lmtdlE-hU",
        "colab_type": "text"
      },
      "source": [
        "Create a CSV submission file , zip and download it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xnZkjgU6eRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating DataFrame with the probability prediction for each segment\n",
        "submission =  pd.DataFrame()\n",
        "submission['segment_id'] = test_df['segment_id']\n",
        "submission['prediction'] = model.predict(test_x)\n",
        "submission['prediction'] = submission['prediction'].astype('float')\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jheQSZzg_Lkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download zip file\n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "\n",
        "with ZipFile('submission.zip', 'w') as myzip:\n",
        "  myzip.write('submission.csv')\n",
        "\n",
        "files.download('submission.zip')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cYkuOvTe3bd",
        "colab_type": "text"
      },
      "source": [
        "# Results Investigation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmTrQnThe64e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "cm = confusion_matrix(y_true=val_y, y_pred=(model.predict(val_x)>0.5)*1)\n",
        "cm_plot_labels = ['human','animal']\n",
        "\n",
        "utils.plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=f\"Confusion Matrix threshold={threshold}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtcSQNiBqhaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_val = (model.predict(val_x).flatten()>0.5)*1\n",
        "df_idx = list(train_dff[train_dff.is_validation].index)\n",
        "results_val = pd.DataFrame({'true':val_y, 'pred':pred_val,'df_idx':df_idx})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namKgFAEpYxB",
        "colab_type": "text"
      },
      "source": [
        "‘target_type’ –  ‘human’ (1) or ‘animal’ (0) - the identified object in the segment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gTUY05EdEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# true=0, pred=0 => GOOD: animals identified correctly as animals\n",
        "\n",
        "print(\"GOOD: animals identified correctly as animals\")\n",
        "\n",
        "num_results =10\n",
        "\n",
        "fig, axarr = plt.subplots(1, num_results,figsize=(2*num_results, 6*num_results))\n",
        "\n",
        "for i in range(num_results):\n",
        "  ax1= axarr[i]\n",
        "  sample = results_val[(results_val.true==0) & (results_val.pred==0)]\n",
        "  ind = sample.iloc[i].df_idx\n",
        "  x_tmp = train_df['iq_sweep_burst'][ind]\n",
        "  utils.plot_spectrogram(\n",
        "      train_df['iq_sweep_burst'][ind],\n",
        "      train_df['doppler_burst'][ind], \n",
        "      color_map_path='/content/cmap.npy',\n",
        "      ax=ax1\n",
        "      )\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8cFeV8ZzLE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# true=1, pred=0 => WRONG: humans mis-identified as animals\n",
        "\n",
        "print(\"WRONG: humans mis-identified as animals\")\n",
        "\n",
        "fig, axarr = plt.subplots(1, num_results,figsize=(2*num_results, 6*num_results))\n",
        "#fig.suptitle(\"WRONG: humans mis-identified as animals\", fontsize=16)\n",
        "\n",
        "for i in range(num_results):\n",
        "  ax1= axarr[i]\n",
        "  sample = results_val[(results_val.true==1) & (results_val.pred==0)]\n",
        "  ind = sample.iloc[i].df_idx\n",
        "  x_tmp = train_df['iq_sweep_burst'][ind]\n",
        "  utils.plot_spectrogram(\n",
        "      train_df['iq_sweep_burst'][ind],\n",
        "      train_df['doppler_burst'][ind], \n",
        "      color_map_path='/content/cmap.npy',\n",
        "      ax=ax1\n",
        "      )\n",
        "  \n",
        "fig.subplots_adjust(top=0.88)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2v11tv9z09l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# true=0, pred=1 => WRONG: animals mis-identified as humans\n",
        "\n",
        "print(\"WRONG: animals mis-identified as humans\")\n",
        "\n",
        "fig, axarr = plt.subplots(1, num_results,figsize=(2*num_results, 6*num_results))\n",
        "#fig.suptitle(\"WRONG: animals mis-identified as humans\", fontsize=16)\n",
        "\n",
        "for i in range(num_results):\n",
        "  ax1= axarr[i]\n",
        "  sample = results_val[(results_val.true==0) & (results_val.pred==1)]\n",
        "  ind = sample.iloc[i].df_idx\n",
        "  x_tmp = train_df['iq_sweep_burst'][ind]\n",
        "  utils.plot_spectrogram(\n",
        "      train_df['iq_sweep_burst'][ind],\n",
        "      train_df['doppler_burst'][ind], \n",
        "      color_map_path='/content/cmap.npy',\n",
        "      ax=ax1\n",
        "      )\n",
        "  \n",
        "fig.subplots_adjust(top=0.88)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "458LqHtA3sh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# true=1, pred=1  => GOOD: humans identified correctly as humans.\n",
        "\n",
        "print(\"GOOD: humans identified correctly as humans\")\n",
        "\n",
        "rows_results = 4\n",
        "num_results=40\n",
        "\n",
        "fig, axarr = plt.subplots(rows_results, \n",
        "                          int(num_results/rows_results),\n",
        "                          figsize=( 20, 5*rows_results )\n",
        ")\n",
        "#fig.suptitle(\"GOOD: humans identified correctly as humans\", fontsize=16)\n",
        "\n",
        "for i in range(num_results):\n",
        "  ax1= axarr[int(i%rows_results),int(i/rows_results)]\n",
        "  sample = results_val[(results_val.true==1) & (results_val.pred==1)]\n",
        "  ind = sample.iloc[i].df_idx\n",
        "  x_tmp = train_df['iq_sweep_burst'][ind]\n",
        "  utils.plot_spectrogram(\n",
        "      train_df['iq_sweep_burst'][ind],\n",
        "      train_df['doppler_burst'][ind], \n",
        "      color_map_path='/content/cmap.npy',\n",
        "      ax=ax1\n",
        "      )\n",
        "  \n",
        "fig.subplots_adjust(top=0.88)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoXlalR__YhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}