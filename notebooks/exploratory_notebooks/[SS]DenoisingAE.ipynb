{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitvenvmafatvenv300c218a63dc4f9ea0765cbe246bbed9",
   "display_name": "Python 3.6.9 64-bit ('venv_mafat': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path\n",
    "from os import path\n",
    "from importlib import reload\n",
    "import wandb\n",
    "\n",
    "\n",
    "creds_path_ar = [\"../../credentials.ini\",\"credentials.colab.ini\"]\n",
    "PATH_ROOT = \"\"\n",
    "PATH_DATA = \"\"\n",
    "\n",
    "for creds_path in creds_path_ar:\n",
    "    if path.exists(creds_path):\n",
    "        config_parser = configparser.ConfigParser()\n",
    "        config_parser.read(creds_path)\n",
    "        PATH_ROOT = config_parser['MAIN'][\"PATH_ROOT\"]\n",
    "        PATH_DATA = config_parser['MAIN'][\"PATH_DATA\"]\n",
    "        WANDB_enable = config_parser['MAIN'][\"WANDB_ENABLE\"] == 'TRUE'\n",
    "        ENV = config_parser['MAIN'][\"ENV\"]\n",
    "        break\n",
    "\n",
    "if ENV==\"COLAB\":\n",
    "  from google.colab import drive\n",
    "  mount_path = '/content/gdrive/'\n",
    "  drive.mount(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/shaul/workspace/GitHub/sota-mafat-radar\n"
    }
   ],
   "source": [
    "cd {PATH_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from termcolor import colored\n",
    "\n",
    "from src.data import feat_data, get_data\n",
    "from src.features import specto_feat\n",
    "from src.models import arch_setup, base_base_model, alex_model, tcn_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for reproducibility of results\n",
    "seed_value = 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['num_tracks'] = 4\n",
    "config['val_ratio'] = 6\n",
    "config['shift_segment'] = np.arange(1,32)\n",
    "config['get_shifts'] = False\n",
    "config['get_horizontal_flip'] = False\n",
    "config['get_vertical_flip'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_x, train_y, val_x, val_y = get_data.classic_trainval(PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'MAFAT RADAR Challenge - Training Set V1'\n",
    "training_dict = get_data.load_data(train_path, PATH_DATA)\n",
    "\n",
    "#split Tracks here to only do augmentation on Train set\n",
    "train_dict, val_dict = get_data.split_train_val_as_df(training_dict,ratio= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data = get_data.load_data(PATH_ROOT + PATH_DATA + 'MAFAT RADAR Challenge - Auxiliary Synthetic Set V2')\n",
    "synth_data['segment_id'] = np.array(synth_data['segment_id'].tolist()) - 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-46b7d0fd5f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segment_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-46b7d0fd5f9e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segment_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "[x['iq_sweep_burst'] for x in train_dict if x['segment_id'] in id_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 6347 is out of bounds for axis 0 with size 6347",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3bb754128d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mid_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msynth_train_xhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynth_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msynth_train_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 6347 is out of bounds for axis 0 with size 6347"
     ]
    }
   ],
   "source": [
    "id_train = list(set(synth_data['segment_id']).intersection(set(train_dict['segment_id'])))\n",
    "id_train.sort()\n",
    "synth_train_xhat = synth_data['iq_sweep_burst'][id_train]\n",
    "synth_train_x = train_dict['iq_sweep_burst'][id_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(6347,)"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "train_dict['segment_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6567"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "max(list(set(train_dict['segment_id']).intersection(set(synth_data['segment_id']))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_val = list(set(synth_data['segment_id']).intersection(set(val_dict['segment_id'])))\n",
    "id_val.sort()\n",
    "synth_val = synth_data['iq_sweep_burst'][id_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self,noisy_df,clean_df):\n",
    "        super().__init__()\n",
    "        self.noisy_df = noisy_df\n",
    "        self.clean_df = clean_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.noisy_df.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.noisy_df[idx]\n",
    "        x_hat = self.clean_df[idx]\n",
    "        return x, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DS(synth_train,train_dict['iq_sweep_burst'])\n",
    "val_set = DS(synth_val, val_dict['iq_sweep_burst'])\n",
    "train_loader = DataLoader(dataset= train_set, batch_size = 32, shuffle = True, num_workers = 2)\n",
    "val_loader = DataLoader(dataset= val_set, batch_size = 32, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# DeConv1\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "# DeConv2\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "# Deconv3\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "'''\n",
    "\n",
    "# class autoencoder(nn.Module):\n",
    "    # def __init__(self):\n",
    "    #     super(autoencoder, self).__init__()\n",
    "    #     self.encoder = nn.Sequential(\n",
    "    #         nn.Conv2d(1,16, kernel_size=(3,3),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Conv2d(32,64, kernel_size=(5,5),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Conv2d(64,128, kernel_size=(5,5),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Linear(128*32*128, 64),\n",
    "    #         nn.ReLU(True))\n",
    "    #     self.decoder = nn.Sequential(\n",
    "    #         nn.ConvTranspose2d(1,16,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(16,32,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(32,64,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(64,1,2,2),\n",
    "    #         nn.Sigmoid())\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.encoder(x)\n",
    "    #     x = self.decoder(x)\n",
    "    #     return x\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "       \n",
    "        #Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "              \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder().to(device)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "  0%|          | 0/71 [00:00<?, ?it/s]Entering Epoch:  0\n  0%|          | 0/71 [00:00<?, ?it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in <listcomp>\n    return default_collate([torch.as_tensor(b) for b in batch])\nTypeError: can't convert np.ndarray of type numpy.complex128. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a2e477d08172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entering Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mdirty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in <listcomp>\n    return default_collate([torch.as_tensor(b) for b in batch])\nTypeError: can't convert np.ndarray of type numpy.complex128. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "l=len(train_loader)\n",
    "losslist=list()\n",
    "epochloss=0\n",
    "running_loss=0\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  print(\"Entering Epoch: \",epoch)\n",
    "  for dirty,clean in tqdm((train_loader)):\n",
    "    \n",
    "    \n",
    "    # dirty=dirty.view(dirty.size(0),-1).type(torch.FloatTensor)\n",
    "    # clean=clean.view(clean.size(0),-1).type(torch.FloatTensor)\n",
    "    dirty = torch.unsqueeze(dirty,1)\n",
    "    clean = torch.unsqueeze(clean,1)\n",
    "    dirty,clean=dirty.to(device),clean.to(device)\n",
    "    \n",
    "    #-----------------Forward Pass----------------------\n",
    "    output=model(dirty)\n",
    "    loss=criterion(output,clean)\n",
    "    #-----------------Backward Pass---------------------\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss+=loss.item()\n",
    "    epochloss+=loss.item()\n",
    "  #-----------------Log-------------------------------\n",
    "  losslist.append(running_loss/l)\n",
    "  running_loss=0\n",
    "  print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting torch==1.5.1\n  Using cached torch-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (753.2 MB)\nRequirement already satisfied: future in ./venv_mafat/lib/python3.6/site-packages (from torch==1.5.1) (0.18.2)\nRequirement already satisfied: numpy in ./venv_mafat/lib/python3.6/site-packages (from torch==1.5.1) (1.18.5)\n\u001b[31mERROR: torchvision 0.7.0 has requirement torch==1.6.0, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\nInstalling collected packages: torch\n  Attempting uninstall: torch\n    Found existing installation: torch 1.6.0\n    Uninstalling torch-1.6.0:\n      Successfully uninstalled torch-1.6.0\nSuccessfully installed torch-1.5.1\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "# !pip install torch==1.5.1"
   ]
  },
  {
   "source": [
    "# NO MANDS LAND"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([47, 64, 67, 67,  9, 83, 21, 36, 87, 70, 88, 88, 12, 58, 65])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = dict()\n",
    "dict2 = dict()\n",
    "dict1['id'] = list(range(10))\n",
    "dict1['val'] = np.random.rand(10,12)\n",
    "dict2['id'] = list(range(5,15))\n",
    "dict2['val'] = np.random.randint(0,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts(train_dict, synth_dict):\n",
    "    td = dict()\n",
    "    sd = dict()\n",
    "\n",
    "    seg_id = train_dict['segment_id']\n",
    "    iq_sb = train_dict['iq_sweep_burst']\n",
    "\n",
    "    for a,b in zip(seg_id,iq_sb):\n",
    "        td[a] = b\n",
    "\n",
    "    seg_id = synth_dict['segment_id']\n",
    "    iq_sb = synth_dict['iq_sweep_burst']\n",
    "\n",
    "    for a,b in zip(seg_id,iq_sb):\n",
    "        sd[a] = b\n",
    "\n",
    "    comb_dict = dict()\n",
    "\n",
    "    for key1,val1 in td.items():\n",
    "        for key2, val2 in sd.items():\n",
    "            if key1 == key2:\n",
    "                comb_dict[key1] = dict()\n",
    "                comb_dict[key1]['train_iq'] = val1\n",
    "                comb_dict[key1]['synth_iq'] = val2\n",
    "    \n",
    "    return comb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = combine_dicts(train_dict,synth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6567"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "cd['segment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = dict()\n",
    "sd = dict()\n",
    "\n",
    "seg_id = train_dict['segment_id']\n",
    "iq_sb = train_dict['iq_sweep_burst']\n",
    "\n",
    "for a,b in zip(seg_id,iq_sb):\n",
    "    td[a] = b\n",
    "\n",
    "seg_id = synth_data['segment_id']\n",
    "iq_sb = synth_data['iq_sweep_burst']\n",
    "\n",
    "for a,b in zip(seg_id,iq_sb):\n",
    "    sd[a] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-22630c7f29f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcomb_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "comb_dict = dict()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for key1,val1 in td.items():\n",
    "    for key2, val2 in sd.items():\n",
    "        if key1 == key2:\n",
    "            i = i +1\n",
    "            comb_dict[key1] = dict()\n",
    "            comb_dict[key1]['train_iq'] = val1\n",
    "            comb_dict[key1]['synth_iq'] = val2\n",
    "            if i == 2:\n",
    "                print(comb_dict)\n",
    "                break\n",
    "return comb_dict"
   ]
  }
 ]
}