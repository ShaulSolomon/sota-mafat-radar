{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitvenvmafatvenv300c218a63dc4f9ea0765cbe246bbed9",
   "display_name": "Python 3.6.9 64-bit ('venv_mafat': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path\n",
    "from os import path\n",
    "from importlib import reload\n",
    "import wandb\n",
    "\n",
    "\n",
    "creds_path_ar = [\"../../credentials.ini\",\"credentials.colab.ini\"]\n",
    "PATH_ROOT = \"\"\n",
    "PATH_DATA = \"\"\n",
    "\n",
    "for creds_path in creds_path_ar:\n",
    "    if path.exists(creds_path):\n",
    "        config_parser = configparser.ConfigParser()\n",
    "        config_parser.read(creds_path)\n",
    "        PATH_ROOT = config_parser['MAIN'][\"PATH_ROOT\"]\n",
    "        PATH_DATA = config_parser['MAIN'][\"PATH_DATA\"]\n",
    "        WANDB_enable = config_parser['MAIN'][\"WANDB_ENABLE\"] == 'TRUE'\n",
    "        ENV = config_parser['MAIN'][\"ENV\"]\n",
    "        break\n",
    "\n",
    "if ENV==\"COLAB\":\n",
    "  from google.colab import drive\n",
    "  mount_path = '/content/gdrive/'\n",
    "  drive.mount(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/shaul/workspace/GitHub/sota-mafat-radar\n"
    }
   ],
   "source": [
    "cd {PATH_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from termcolor import colored\n",
    "\n",
    "from src.data import feat_data, get_data\n",
    "from src.features import specto_feat\n",
    "from src.models import arch_setup, base_base_model, alex_model, tcn_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for reproducibility of results\n",
    "seed_value = 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['num_tracks'] = 4\n",
    "config['val_ratio'] = 6\n",
    "config['shift_segment'] = np.arange(1,32)\n",
    "config['get_shifts'] = False\n",
    "config['get_horizontal_flip'] = False\n",
    "config['get_vertical_flip'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_x, train_y, val_x, val_y = get_data.classic_trainval(PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'MAFAT RADAR Challenge - Training Set V1'\n",
    "training_dict = get_data.load_data(train_path, PATH_DATA)\n",
    "\n",
    "#split Tracks here to only do augmentation on Train set\n",
    "train_dict, val_dict = get_data.split_train_val_as_df(training_dict,ratio= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data = get_data.load_data(PATH_ROOT + PATH_DATA + 'MAFAT RADAR Challenge - Auxiliary Synthetic Set V2')\n",
    "synth_data['segment_id'] = np.array(synth_data['segment_id'].tolist()) - 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-46b7d0fd5f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segment_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-46b7d0fd5f9e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segment_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "[x['iq_sweep_burst'] for x in train_dict if x['segment_id'] in id_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 6347 is out of bounds for axis 0 with size 6347",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3bb754128d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mid_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msynth_train_xhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynth_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msynth_train_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iq_sweep_burst'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 6347 is out of bounds for axis 0 with size 6347"
     ]
    }
   ],
   "source": [
    "id_train = list(set(synth_data['segment_id']).intersection(set(train_dict['segment_id'])))\n",
    "id_train.sort()\n",
    "synth_train_xhat = synth_data['iq_sweep_burst'][id_train]\n",
    "synth_train_x = train_dict['iq_sweep_burst'][id_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(6347,)"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "train_dict['segment_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6567"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "max(list(set(train_dict['segment_id']).intersection(set(synth_data['segment_id']))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_val = list(set(synth_data['segment_id']).intersection(set(val_dict['segment_id'])))\n",
    "id_val.sort()\n",
    "synth_val = synth_data['iq_sweep_burst'][id_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self,noisy_df,clean_df):\n",
    "        super().__init__()\n",
    "        self.noisy_df = noisy_df\n",
    "        self.clean_df = clean_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.noisy_df.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.noisy_df[idx]\n",
    "        x_hat = self.clean_df[idx]\n",
    "        return x, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DS(synth_train,train_dict['iq_sweep_burst'])\n",
    "val_set = DS(synth_val, val_dict['iq_sweep_burst'])\n",
    "train_loader = DataLoader(dataset= train_set, batch_size = 32, shuffle = True, num_workers = 2)\n",
    "val_loader = DataLoader(dataset= val_set, batch_size = 32, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# DeConv1\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "# DeConv2\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "# Deconv3\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "'''\n",
    "\n",
    "# class autoencoder(nn.Module):\n",
    "    # def __init__(self):\n",
    "    #     super(autoencoder, self).__init__()\n",
    "    #     self.encoder = nn.Sequential(\n",
    "    #         nn.Conv2d(1,16, kernel_size=(3,3),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Conv2d(32,64, kernel_size=(5,5),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Conv2d(64,128, kernel_size=(5,5),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Linear(128*32*128, 64),\n",
    "    #         nn.ReLU(True))\n",
    "    #     self.decoder = nn.Sequential(\n",
    "    #         nn.ConvTranspose2d(1,16,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(16,32,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(32,64,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(64,1,2,2),\n",
    "    #         nn.Sigmoid())\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.encoder(x)\n",
    "    #     x = self.decoder(x)\n",
    "    #     return x\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "       \n",
    "        #Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "              \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder().to(device)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "  0%|          | 0/71 [00:00<?, ?it/s]Entering Epoch:  0\n  0%|          | 0/71 [00:00<?, ?it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in <listcomp>\n    return default_collate([torch.as_tensor(b) for b in batch])\nTypeError: can't convert np.ndarray of type numpy.complex128. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a2e477d08172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entering Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mdirty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 64, in <listcomp>\n    return default_collate([torch.as_tensor(b) for b in batch])\nTypeError: can't convert np.ndarray of type numpy.complex128. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "l=len(train_loader)\n",
    "losslist=list()\n",
    "epochloss=0\n",
    "running_loss=0\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  print(\"Entering Epoch: \",epoch)\n",
    "  for dirty,clean in tqdm((train_loader)):\n",
    "    \n",
    "    \n",
    "    # dirty=dirty.view(dirty.size(0),-1).type(torch.FloatTensor)\n",
    "    # clean=clean.view(clean.size(0),-1).type(torch.FloatTensor)\n",
    "    dirty = torch.unsqueeze(dirty,1)\n",
    "    clean = torch.unsqueeze(clean,1)\n",
    "    dirty,clean=dirty.to(device),clean.to(device)\n",
    "    \n",
    "    #-----------------Forward Pass----------------------\n",
    "    output=model(dirty)\n",
    "    loss=criterion(output,clean)\n",
    "    #-----------------Backward Pass---------------------\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss+=loss.item()\n",
    "    epochloss+=loss.item()\n",
    "  #-----------------Log-------------------------------\n",
    "  losslist.append(running_loss/l)\n",
    "  running_loss=0\n",
    "  print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting torch==1.5.1\n  Using cached torch-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (753.2 MB)\nRequirement already satisfied: future in ./venv_mafat/lib/python3.6/site-packages (from torch==1.5.1) (0.18.2)\nRequirement already satisfied: numpy in ./venv_mafat/lib/python3.6/site-packages (from torch==1.5.1) (1.18.5)\n\u001b[31mERROR: torchvision 0.7.0 has requirement torch==1.6.0, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\nInstalling collected packages: torch\n  Attempting uninstall: torch\n    Found existing installation: torch 1.6.0\n    Uninstalling torch-1.6.0:\n      Successfully uninstalled torch-1.6.0\nSuccessfully installed torch-1.5.1\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "# !pip install torch==1.5.1"
   ]
  },
  {
   "source": [
    "# NO MANDS LAND"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([47, 64, 67, 67,  9, 83, 21, 36, 87, 70, 88, 88, 12, 58, 65])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = dict()\n",
    "dict2 = dict()\n",
    "dict1['id'] = list(range(10))\n",
    "dict1['val'] = np.random.rand(10,12)\n",
    "dict2['id'] = list(range(5,15))\n",
    "dict2['val'] = np.random.randint(0,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts(train_dict, synth_dict):\n",
    "    td = dict()\n",
    "    sd = dict()\n",
    "\n",
    "    seg_id = train_dict['segment_id']\n",
    "    iq_sb = train_dict['iq_sweep_burst']\n",
    "\n",
    "    for a,b in zip(seg_id,iq_sb):\n",
    "        td[a] = b\n",
    "\n",
    "    seg_id = synth_dict['segment_id']\n",
    "    iq_sb = synth_dict['iq_sweep_burst']\n",
    "\n",
    "    for a,b in zip(seg_id,iq_sb):\n",
    "        sd[a] = b\n",
    "\n",
    "    comb_dict = dict()\n",
    "\n",
    "    for key1,val1 in td.items():\n",
    "        for key2, val2 in sd.items():\n",
    "            if key1 == key2:\n",
    "                comb_dict[key1] = dict()\n",
    "                comb_dict[key1]['train_iq'] = val1\n",
    "                comb_dict[key1]['synth_iq'] = val2\n",
    "    \n",
    "    return comb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = combine_dicts(train_dict,synth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6567"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "cd['segment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = dict()\n",
    "sd = dict()\n",
    "\n",
    "seg_id = train_dict['segment_id']\n",
    "iq_sb = train_dict['iq_sweep_burst']\n",
    "\n",
    "for a,b in zip(seg_id,iq_sb):\n",
    "    td[a] = b\n",
    "\n",
    "seg_id = synth_data['segment_id']\n",
    "iq_sb = synth_data['iq_sweep_burst']\n",
    "\n",
    "for a,b in zip(seg_id,iq_sb):\n",
    "    sd[a] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{1: {'train_iq': array([[-2723.53403759  -36.07527924j,   639.59323502 -307.21540833j,\n          497.52026749  -91.40984726j, ...,\n         -391.50698662 -810.83663177j,  -819.19118881 +155.6763525j ,\n         -178.84239483 +485.27436829j],\n       [-2682.17344189 +245.80362701j,   333.33102798 -195.3150177j ,\n          855.54663467 +663.68194962j, ...,\n         -594.34365654 -655.54561615j, -1344.311306    -58.51578617j,\n         -239.93492413  +99.72163391j],\n       [-3041.93369579 +173.88272858j,   834.00046158 -345.21540833j,\n          390.42261124 +313.81671524j, ...,\n         -465.46444511 -605.45088959j, -1217.24636459 +306.41756344j,\n         -241.76988506 +141.37202454j],\n       ...,\n       [ 1259.4869585  -344.35262299j,  -514.71462631  +80.93009949j,\n         -735.53783798 +453.86310196j, ...,\n          196.55014229 -246.58663177j,  -383.77322006 +535.92171383j,\n           99.65491962-1120.57524109j],\n       [ 1636.63368702 -720.18270111j,  -830.22561264  +21.3802948j ,\n        -1592.36645126 +243.62286758j, ...,\n          115.91549873  +84.57743073j,    68.34250259 -151.96061039j,\n          -23.55308819 -918.09086609j],\n       [ 2052.24062061 -601.84090424j,  -514.17361069 -110.73005676j,\n        -1813.84692001 -106.04461288j, ...,\n          117.90353584  -83.98604584j,    45.52804947  +72.60970211j,\n         -200.51085186 -881.11430359j]]), 'synth_iq': array([[-4061.00997616 +590.06943013j,   151.66106503-1424.26540088j,\n          272.93513162 -734.14821662j, ...,\n         -967.44774303  +39.90134583j, -1671.1856109  +842.6321536j ,\n         -991.82721146  +15.38253856j],\n       [-3063.70340049 +673.25989838j,   116.26541347 +451.76417534j,\n         1340.32007128+1219.19998219j, ...,\n          295.22245368 +527.04232825j, -1549.16132676 -879.35772264j,\n           79.78990585  -75.1791286j ],\n       [-3633.24473759 +245.61391447j,  1546.60375226 +733.21385154j,\n          396.07495549 +888.0635121j , ...,\n        -1510.48493435 -726.78636227j, -1720.26115224 -421.87885067j,\n         -618.53157597 +363.07568828j],\n       ...,\n       [  155.75174048 -271.3612121j ,   788.98466121+1365.27313327j,\n        -1736.29032363 -267.0823299j , ...,\n        -1105.77814488+1468.37466346j, -1477.48110319+1437.80993457j,\n         -687.25933595-1384.26402306j],\n       [ 2691.43859548 +238.70483789j, -1534.30834078 -200.33206237j,\n        -1768.39910046 +429.4229231j , ...,\n         -126.10154158-1042.64255209j,  -381.26567077  -64.06133242j,\n         1777.77205929-2217.66626751j],\n       [ 2285.91693568-1645.83825038j,  -808.1215357  -445.78838659j,\n        -1605.59424749 +408.5997324j , ...,\n          578.27690064 -361.96592034j,   871.36282791 +383.22207602j,\n          142.16025103-1011.38738484j]])}, 2: {'train_iq': array([[2121.38956451-1027.16182613j,  159.70161819+1775.17122555j,\n        -590.60393858  -88.49250793j, ..., -997.93901825  -76.49578476j,\n         -84.36696625 +310.82037735j,  821.58823776 +207.70370483j],\n       [2119.01993561 -765.92208004j, -309.14213181+1885.22579098j,\n        -251.76372862 -282.02180481j, ..., -277.91655731 -146.25359726j,\n        -282.5842514  +187.56695938j,  695.29673386 -838.46524048j],\n       [2210.44669342 -976.85567379j,  -81.18998337+1718.63082027j,\n        -602.02233458 -148.24250793j, ..., -113.84966278 -669.38152695j,\n        -512.08571625 +282.81354141j,  432.27036667-1313.1468811j ],\n       ...,\n       [-765.58846283 +329.57474613j, -234.90287399 -131.56278324j,\n         729.66406679 -472.73469543j, ...,   78.87445831  -28.1744957j ,\n        -834.24684906 -677.75042343j,   19.73814011 -695.77676392j],\n       [-739.14998627 +350.41703129j, -131.38724899   +8.90474606j,\n         749.56689882 -247.97297668j, ...,  130.91156769 +434.33917618j,\n        -528.93386078 -459.85491562j, -458.6627388  +223.35946655j],\n       [-983.6465683  +547.62040043j,   49.22505569 -491.30448246j,\n         596.59247255 -268.37141418j, ...,  173.66791534 +183.34991837j,\n         -52.29958344 +518.71637344j, -794.30727005 +894.8711853j ]]), 'synth_iq': array([[ 3094.27937293-1273.94692808j,   495.88949566 +890.20176788j,\n         -995.46483091 +142.73207529j, ...,\n        -1225.96249245 +609.53333743j, -1235.35464109 -475.26221304j,\n          -58.91515741 -505.12953513j],\n       [ 1505.82059204-1555.89255462j,   207.4774401 +3020.67529811j,\n         -302.54681237  -46.21185512j, ...,\n          168.31712491  -72.13526436j,  -617.53695544 +360.24180786j,\n         -145.27468327 +187.25446855j],\n       [ 1791.34848583 -857.09253257j,  1452.93746994+2199.37619117j,\n         -602.47914866 +581.39732249j, ...,\n           82.59143658-2571.59237679j, -1257.92633748 -420.47860482j,\n         1496.35388024-1500.33870312j],\n       ...,\n       [ -888.99066295 +873.50676935j,  -489.86168139 -358.50083186j,\n         -351.42439779-1452.21561217j, ...,\n          986.11201697 -435.00573817j,   280.00432522 -864.14444066j,\n          107.76927541-1897.49408638j],\n       [ -619.64698572+1854.25398044j,   372.34529878-1087.11745899j,\n         1237.6797533  -902.9671389j , ...,\n        -1286.5616764  -167.93883861j,  -474.48594184-1180.71819353j,\n          284.54494499 +717.56285823j],\n       [ -995.552711   +477.93923119j,  1009.85596162 -339.47189767j,\n          963.08355477 -842.42168372j, ...,\n          348.12943657 -491.39367529j,  -674.61115666 +228.83158916j,\n         -427.27396366 +163.86985153j]])}}\n"
    },
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-76-5f0ed0838be1>, line 15)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-76-5f0ed0838be1>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    return comb_dict\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "comb_dict = dict()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for key1,val1 in td.items():\n",
    "    for key2, val2 in sd.items():\n",
    "        if key1 == key2:\n",
    "            i = i +1\n",
    "            comb_dict[key1] = dict()\n",
    "            comb_dict[key1]['train_iq'] = val1\n",
    "            comb_dict[key1]['synth_iq'] = val2\n",
    "            if i == 2:\n",
    "                print(comb_dict)\n",
    "                break\n",
    "return comb_dict"
   ]
  }
 ]
}