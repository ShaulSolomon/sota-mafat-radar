{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitvenvmafatvenv300c218a63dc4f9ea0765cbe246bbed9",
   "display_name": "Python 3.6.9 64-bit ('venv_mafat': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path\n",
    "from os import path\n",
    "from importlib import reload\n",
    "import wandb\n",
    "\n",
    "\n",
    "creds_path_ar = [\"../../credentials.ini\",\"credentials.colab.ini\"]\n",
    "PATH_ROOT = \"\"\n",
    "PATH_DATA = \"\"\n",
    "\n",
    "for creds_path in creds_path_ar:\n",
    "    if path.exists(creds_path):\n",
    "        config_parser = configparser.ConfigParser()\n",
    "        config_parser.read(creds_path)\n",
    "        PATH_ROOT = config_parser['MAIN'][\"PATH_ROOT\"]\n",
    "        PATH_DATA = config_parser['MAIN'][\"PATH_DATA\"]\n",
    "        WANDB_enable = config_parser['MAIN'][\"WANDB_ENABLE\"] == 'TRUE'\n",
    "        ENV = config_parser['MAIN'][\"ENV\"]\n",
    "        break\n",
    "\n",
    "if ENV==\"COLAB\":\n",
    "  from google.colab import drive\n",
    "  mount_path = '/content/gdrive/'\n",
    "  drive.mount(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/shaul/workspace/GitHub/sota-mafat-radar\n"
    }
   ],
   "source": [
    "cd {PATH_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from termcolor import colored\n",
    "\n",
    "from src.data import feat_data, get_data\n",
    "from src.features import specto_feat\n",
    "from src.models import arch_setup, base_base_model, alex_model, tcn_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for reproducibility of results\n",
    "seed_value = 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['num_tracks'] = 4\n",
    "config['val_ratio'] = 6\n",
    "config['shift_segment'] = np.arange(1,32)\n",
    "config['get_shifts'] = False\n",
    "config['get_horizontal_flip'] = False\n",
    "config['get_vertical_flip'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[32mEverything is setup correctly\u001b[0m\n"
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = get_data.classic_trainval(PATH_ROOT+PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'MAFAT RADAR Challenge - Training Set V1'\n",
    "training_dict = get_data.load_data(train_path, PATH_DATA)\n",
    "\n",
    "#split Tracks here to only do augmentation on Train set\n",
    "train_dict, val_dict = get_data.split_train_val_as_df(training_dict,ratio= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data = get_data.load_data(PATH_ROOT + PATH_DATA + 'MAFAT RADAR Challenge - Auxiliary Synthetic Set V2')\n",
    "synth_data['segment_id'] = np.array(synth_data['segment_id'].tolist()) - 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train = list(set(synth_data['segment_id']).intersection(set(train_dict['segment_id'])))\n",
    "id_train.sort()\n",
    "synth_train = synth_data['iq_sweep_burst'][id_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_val = list(set(synth_data['segment_id']).intersection(set(val_dict['segment_id'])))\n",
    "id_val.sort()\n",
    "synth_val = synth_data['iq_sweep_burst'][id_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self,noisy_df,clean_df):\n",
    "        super().__init__()\n",
    "        self.noisy_df = noisy_df\n",
    "        self.clean_df = clean_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.noisy_df.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.noisy_df[idx]\n",
    "        x_hat = self.clean_df[idx]\n",
    "        return x, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DS(synth_train,train_dict['iq_sweep_burst'])\n",
    "val_set = DS(synth_val, val_dict['iq_sweep_burst'])\n",
    "train_loader = DataLoader(dataset= train_set, batch_size = 32, shuffle = True, num_workers = 2)\n",
    "val_loader = DataLoader(dataset= val_set, batch_size = 32, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# DeConv1\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "# DeConv2\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "\n",
    "# Deconv3\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "'''\n",
    "\n",
    "# class autoencoder(nn.Module):\n",
    "    # def __init__(self):\n",
    "    #     super(autoencoder, self).__init__()\n",
    "    #     self.encoder = nn.Sequential(\n",
    "    #         nn.Conv2d(1,16, kernel_size=(3,3),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Conv2d(32,64, kernel_size=(5,5),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Conv2d(64,128, kernel_size=(5,5),padding=(2,2)),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.MaxPool2d(kernel_size=(2,2),padding=(2,2)),\n",
    "    #         nn.Linear(128*32*128, 64),\n",
    "    #         nn.ReLU(True))\n",
    "    #     self.decoder = nn.Sequential(\n",
    "    #         nn.ConvTranspose2d(1,16,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(16,32,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(32,64,3,2),\n",
    "    #         nn.ReLU(True),\n",
    "    #         nn.ConvTranspose2d(64,1,2,2),\n",
    "    #         nn.Sigmoid())\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.encoder(x)\n",
    "    #     x = self.decoder(x)\n",
    "    #     return x\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "       \n",
    "        #Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "              \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility of results\n",
    "seed_value = 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder().to(device)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "  0%|          | 0/71 [00:00<?, ?it/s]Entering Epoch:  0\n  0%|          | 0/71 [00:00<?, ?it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "\"unfolded2d_copy\" not implemented for 'ComplexDouble'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-a2e477d08172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#-----------------Forward Pass----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#-----------------Backward Pass---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-233-75aa32f4f39a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/GitHub/sota-mafat-radar/venv_mafat/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"unfolded2d_copy\" not implemented for 'ComplexDouble'"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "l=len(train_loader)\n",
    "losslist=list()\n",
    "epochloss=0\n",
    "running_loss=0\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  print(\"Entering Epoch: \",epoch)\n",
    "  for dirty,clean in tqdm((train_loader)):\n",
    "    \n",
    "    \n",
    "    # dirty=dirty.view(dirty.size(0),-1).type(torch.FloatTensor)\n",
    "    # clean=clean.view(clean.size(0),-1).type(torch.FloatTensor)\n",
    "    dirty = torch.unsqueeze(dirty,1)\n",
    "    clean = torch.unsqueeze(clean,1)\n",
    "    dirty,clean=dirty.to(device),clean.to(device)\n",
    "    \n",
    "    #-----------------Forward Pass----------------------\n",
    "    output=model(dirty)\n",
    "    loss=criterion(output,clean)\n",
    "    #-----------------Backward Pass---------------------\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss+=loss.item()\n",
    "    epochloss+=loss.item()\n",
    "  #-----------------Log-------------------------------\n",
    "  losslist.append(running_loss/l)\n",
    "  running_loss=0\n",
    "  print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting torch==1.5.1\n  Using cached torch-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (753.2 MB)\nRequirement already satisfied: future in ./venv_mafat/lib/python3.6/site-packages (from torch==1.5.1) (0.18.2)\nRequirement already satisfied: numpy in ./venv_mafat/lib/python3.6/site-packages (from torch==1.5.1) (1.18.5)\n\u001b[31mERROR: torchvision 0.7.0 has requirement torch==1.6.0, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\nInstalling collected packages: torch\n  Attempting uninstall: torch\n    Found existing installation: torch 1.6.0\n    Uninstalling torch-1.6.0:\n      Successfully uninstalled torch-1.6.0\nSuccessfully installed torch-1.5.1\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/home/shaul/workspace/GitHub/sota-mafat-radar/venv_mafat/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "!pip install torch==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}