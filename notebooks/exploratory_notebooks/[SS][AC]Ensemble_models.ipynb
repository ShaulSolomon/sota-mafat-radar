{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.10 64-bit",
   "display_name": "Python 3.6.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4c1e195df8d07db5ee7a78f454b46c3f2e14214bf8c9489d2db5cf8f372ff2ed"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "PATH_ROOT = \"\"\n",
    "PATH_DATA = \"\"\n",
    "\n",
    "creds_path_ar = [\"../../credentials.ini\", \"credentials.ini\"]\n",
    "\n",
    "for creds_path in creds_path_ar:\n",
    "    if path.exists(creds_path):\n",
    "        config_parser = configparser.ConfigParser()\n",
    "        config_parser.read(creds_path)\n",
    "        PATH_ROOT = config_parser['MAIN'][\"PATH_ROOT\"]\n",
    "        PATH_DATA = config_parser['MAIN'][\"PATH_DATA\"]\n",
    "        WANDB_enable = config_parser['MAIN'][\"WANDB_ENABLE\"] == 'TRUE'\n",
    "        ENV = config_parser['MAIN'][\"ENV\"]\n",
    "\n",
    "# adding cwd to path to avoid \"No module named src.*\" errors\n",
    "sys.path.insert(0, os.path.join(PATH_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from src.data import get_data\n",
    "from src.data.iterable_dataset import Config, DataDict, StreamingDataset, iq_to_spectogram, \\\n",
    "    normalize\n",
    "from src.models import arch_setup, tcn_model3\n",
    "from src.data import get_data\n",
    "from src.visualization import metrics\n",
    "from src.features import specto_feat\n",
    "import wandb\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/ubuntu/sota-mafat-radar\n"
     ]
    }
   ],
   "source": [
    "cd {PATH_ROOT}\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'src.visualization.metrics' from '/home/ubuntu/sota-mafat-radar/src/visualization/metrics.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(metrics)"
   ]
  },
  {
   "source": [
    "# Two Parts\n",
    "\n",
    "1. Run off the test data to get the test scores for a given model (to give us some indication of the accuracy)\n",
    "2. With the various models, create a LR using the test data as its own train/val."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### PART 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path: str):\n",
    "    '''\n",
    "    Load Model from Wandb\n",
    "    '''\n",
    "    wandb.restore('data/models/model.pth', run_path=model_path)\n",
    "    return torch.load('data/models/model.pth')\n",
    "\n",
    "\n",
    "def load_testset(test_path: str):\n",
    "    '''\n",
    "    Load Test Data\n",
    "    '''\n",
    "    test_data = pd.DataFrame.from_dict(get_data.load_data(test_path, PATH_DATA), orient='index').transpose()\n",
    "    test_data['target_type'].replace({'animal': 0, 'human': 1}, inplace=True)\n",
    "\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def run_predictions(model, test_df):\n",
    "    '''\n",
    "    Have the predictions ready for submission\n",
    "    '''\n",
    "    test_df['output_array'] = test_df['iq_sweep_burst'].progress_apply(iq_to_spectogram)\n",
    "    test_df['output_array'] = test_df.progress_apply(lambda row: specto_feat.max_value_on_doppler(row['output_array'], row['doppler_burst']), axis=1)\n",
    "    test_df['output_array'] = test_df['output_array'].progress_apply(normalize)\n",
    "    test_x = torch.from_numpy(np.stack(test_df['output_array'].tolist(), axis=0).astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu:0')\n",
    "\n",
    "    # Creating DataFrame with the probability prediction for each segment\n",
    "    submission = pd.DataFrame()\n",
    "    submission[['segment_id', 'label']] = test_df[['segment_id', 'target_type']]\n",
    "    submission['prediction'] = model(test_x.to(device)).detach().cpu().numpy()\n",
    "    submission['label'] = test_df['target_type']\n",
    "    return submission, test_df['target_type']\n",
    "\n",
    "def check_model_auc(model_path: str, test_path: str):\n",
    "    '''\n",
    "    1. Load the Model (using load_model())\n",
    "    2. Load the Test Data (using load_testdata())\n",
    "    3. Return the predictionsauc and acc scores of predictions\n",
    "    '''\n",
    "    model = load_model(model_path)\n",
    "    test_df = load_testset(test_path)\n",
    "    predictions, labels = run_predictions(model, test_df)\n",
    "    return metrics.model_scores(labels, predictions['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'sota-mafat/sota-mafat-base/3s0bv1dr'\n",
    "# model = load_model(model_path)\n",
    "# test_path = 'MAFAT RADAR Challenge - FULL Public Test Set V1'\n",
    "# test_dict = load_testset(test_path)\n",
    "# predics = run_predicions(model, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 284/284 [00:00<00:00, 1960.42it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3153.82it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 13200.74it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6203593408775064"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "check_model_auc(model_path,test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['sota-mafat/sota-mafat-base/1epmi6lf','sota-mafat/sota-mafat-base/3s0bv1dr']"
   ]
  },
  {
   "source": [
    "### PART 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [pred1['prediction'],pred2['prediction']]\n",
    "test1 = pd.concat(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(284,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LogR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_mean(model_paths: list, test_path):\n",
    "    preds = []\n",
    "    test_df = load_testset(test_path)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred,_ = run_predictions(model, test_df)\n",
    "        preds.append(pred['prediction'])\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    pred = df.mean(axis=1)\n",
    "    labels = test_df['target_type']\n",
    "    return metrics.model_scores(labels, pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(model_paths: list, test_path):\n",
    "    preds = []\n",
    "    scores = []\n",
    "    test_df = load_testset(test_path)\n",
    "    labels = test_df['target_type']\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred,_ = run_predictions(model, test_df)\n",
    "        preds.append(pred['prediction'])\n",
    "        scores.append(metrics.model_scores(labels,pred['prediction']))\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    scores = np.array(scores)\n",
    "    scores = scores / np.sum(scores)\n",
    "    weighted_mean = (scores*df).mean(axis=1)\n",
    "    print(weighted_mean.shape)\n",
    "    return metrics.model_scores(labels, weighted_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(model_paths: list, test_path):\n",
    "    preds = []\n",
    "    col_names = range(len(model_paths))\n",
    "    test_df = load_testset(test_path)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred,_ = run_predictions(model, test_df)\n",
    "        preds.append(pred['prediction'])\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    df.columns = col_names \n",
    "    labels = test_df['target_type']\n",
    "    X_train, X_test,y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=43)\n",
    "    clf = LogR().fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return metrics.model_scores(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}