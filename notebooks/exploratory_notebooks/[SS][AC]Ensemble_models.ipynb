{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.10 64-bit",
   "display_name": "Python 3.6.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4c1e195df8d07db5ee7a78f454b46c3f2e14214bf8c9489d2db5cf8f372ff2ed"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "PATH_ROOT = \"\"\n",
    "PATH_DATA = \"\"\n",
    "\n",
    "creds_path_ar = [\"../../credentials.ini\", \"credentials.ini\"]\n",
    "\n",
    "for creds_path in creds_path_ar:\n",
    "    if path.exists(creds_path):\n",
    "        config_parser = configparser.ConfigParser()\n",
    "        config_parser.read(creds_path)\n",
    "        PATH_ROOT = config_parser['MAIN'][\"PATH_ROOT\"]\n",
    "        PATH_DATA = config_parser['MAIN'][\"PATH_DATA\"]\n",
    "        WANDB_enable = config_parser['MAIN'][\"WANDB_ENABLE\"] == 'TRUE'\n",
    "        ENV = config_parser['MAIN'][\"ENV\"]\n",
    "\n",
    "# adding cwd to path to avoid \"No module named src.*\" errors\n",
    "sys.path.insert(0, os.path.join(PATH_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from src.data import get_data\n",
    "from src.data.iterable_dataset import Config, DataDict, StreamingDataset, iq_to_spectogram, \\\n",
    "    normalize\n",
    "from src.models import arch_setup, tcn_model3\n",
    "from src.data import get_data\n",
    "from src.visualization import metrics\n",
    "from src.features import specto_feat\n",
    "import wandb\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/ubuntu/sota-mafat-radar\n"
     ]
    }
   ],
   "source": [
    "cd {PATH_ROOT}\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['sota-mafat/sota-mafat-base/1epmi6lf','sota-mafat/sota-mafat-base/3s0bv1dr']\n",
    "test_path = 'MAFAT RADAR Challenge - FULL Public Test Set V1'\n",
    "final_test_path = 'MAFAT RADAR Challenge - Private Test Set V1'\n"
   ]
  },
  {
   "source": [
    "# Two Parts\n",
    "\n",
    "1. Run off the test data to get the test scores for a given model (to give us some indication of the accuracy)\n",
    "2. With the various models, create a LR using the test data as its own train/val."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### PART 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path: str):\n",
    "    '''\n",
    "    Load Model from Wandb\n",
    "    '''\n",
    "    wandb.restore('data/models/model.pth', run_path=model_path)\n",
    "    return torch.load('data/models/model.pth')\n",
    "\n",
    "\n",
    "def load_testset(test_path: str):\n",
    "    '''\n",
    "    Load Test Data\n",
    "    '''\n",
    "    test_data = pd.DataFrame.from_dict(get_data.load_data(test_path, PATH_DATA), orient='index').transpose()\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def run_predictions(model, test_df, final_submission = False):\n",
    "    '''\n",
    "    Have the predictions ready for submission\n",
    "    '''\n",
    "    test_df['output_array'] = test_df['iq_sweep_burst'].progress_apply(iq_to_spectogram)\n",
    "    test_df['output_array'] = test_df.progress_apply(lambda row: specto_feat.max_value_on_doppler(row['output_array'], row['doppler_burst']), axis=1)\n",
    "    test_df['output_array'] = test_df['output_array'].progress_apply(normalize)\n",
    "    test_x = torch.from_numpy(np.stack(test_df['output_array'].tolist(), axis=0).astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu:0')\n",
    "\n",
    "    # Creating DataFrame with the probability prediction for each segment\n",
    "    submission = pd.DataFrame()\n",
    "    submission['segment_id'] = test_df['segment_id']\n",
    "    submission['prediction'] = model(test_x.to(device)).detach().cpu().numpy()\n",
    "    if not final_submission:\n",
    "        test_data['target_type'].replace({'animal': 0, 'human': 1}, inplace=True)\n",
    "        submission['label'] = test_df['target_type']\n",
    "    return submission\n",
    "\n",
    "def check_model_auc(model_path: str, test_path: str):\n",
    "    '''\n",
    "    1. Load the Model (using load_model())\n",
    "    2. Load the Test Data (using load_testdata())\n",
    "    3. Return the predictionsauc and acc scores of predictions\n",
    "    '''\n",
    "    model = load_model(model_path)\n",
    "    test_df = load_testset(test_path)\n",
    "    predictions = run_predictions(model, test_df)\n",
    "    return metrics.model_scores(predictions['label'], predictions['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'sota-mafat/sota-mafat-base/3s0bv1dr'\n",
    "# model = load_model(model_path)\n",
    "# test_path = 'MAFAT RADAR Challenge - FULL Public Test Set V1'\n",
    "# test_dict = load_testset(test_path)\n",
    "# predics = run_predicions(model, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 284/284 [00:00<00:00, 2104.49it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2884.62it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 12849.03it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5217887631526702"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "check_model_auc(model_path,test_path)"
   ]
  },
  {
   "source": [
    "### PART 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LogR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_mean(model_paths: list, test_path, final_submission = False):\n",
    "    preds = []\n",
    "    test_df = load_testset(test_path)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    pred = df.mean(axis=1)\n",
    "    labels = test_df['target_type']\n",
    "    return metrics.model_scores(labels, pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(model_paths: list, test_path, final_submission = False):\n",
    "    preds = []\n",
    "    scores = []\n",
    "    test_df = load_testset(test_path)\n",
    "    labels = test_df['target_type']\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "        scores.append(metrics.model_scores(labels,pred['prediction']))\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    scores = np.array(scores)\n",
    "    scores = scores / np.sum(scores)\n",
    "    weighted_mean = (scores*df).mean(axis=1)\n",
    "    print(weighted_mean.shape)\n",
    "    return metrics.model_scores(labels, weighted_mean), scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(model_paths: list, test_path, final_submission=False):\n",
    "    preds = []\n",
    "    col_names = range(len(model_paths))\n",
    "    test_df = load_testset(test_path)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    df.columns = col_names \n",
    "    labels = test_df['target_type']\n",
    "    X_train, X_test,y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=43)\n",
    "    clf = LogR().fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return metrics.model_scores(y_test, y_pred), clf.coef_, clf.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 284/284 [00:00<00:00, 2088.72it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3161.28it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 12767.50it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2114.76it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3240.84it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 13159.33it/s]\n",
      "(284,)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5217887631526702"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "# weighted_mean(model_paths, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble(model_paths, old_test_path, final_test_path, ensemble_method, final_submission= True):\n",
    "    preds = []\n",
    "    test_df = load_testset(final_test_path)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "    \n",
    "    df = pd.concat(preds, axis=1)\n",
    "\n",
    "    if ensemble_method == \"weighted_mean\":\n",
    "        _, scores = weighted_mean(model_paths, old_test_path, final_submission)\n",
    "        prediction = (scores*df).mean(axis=1)\n",
    "\n",
    "    elif ensemble_method == \"lr_model\":\n",
    "        _, coef, bias = lr_model(model_paths, old_test_path, final_submission)\n",
    "        prediction = (coef*df).mean(axis=1) + bias\n",
    "    else:\n",
    "        prediction = df\n",
    "        \n",
    "    submission = pd.DataFrame()\n",
    "    test_df = pd.DataFrame.from_dict(get_data.load_data(final_test_path, PATH_DATA), orient='index').transpose()\n",
    "    submission['segment_id'] = test_df['segment_id']\n",
    "    submission['prediction'] = prediction\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 248/248 [00:00<00:00, 1594.01it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 3168.86it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 12758.81it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 2103.82it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 2999.77it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 12802.31it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2069.67it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2883.47it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 9576.27it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 1981.87it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3213.32it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 6803.02it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "y_true takes value in {'animal', 'human'} and pos_label is not specified: either make y_true take value in {0, 1} or {-1, 1} or pass pos_label explicitly.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-66b21d422c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_test_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-151-5a7cb9bbbe32>\u001b[0m in \u001b[0;36mrun_ensemble\u001b[0;34m(model_paths, old_test_path, final_test_path, ensemble_method, final_submission)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mensemble_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"lr_model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_test_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_submission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-c695bc280c1c>\u001b[0m in \u001b[0;36mlr_model\u001b[0;34m(model_paths, test_path, final_submission)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/sota-mafat-radar/src/visualization/metrics.py\u001b[0m in \u001b[0;36mmodel_scores\u001b[0;34m(actual, pred)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \"\"\"\n\u001b[1;32m    775\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 776\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    565\u001b[0m                          \u001b[0;34m\"take value in {{0, 1}} or {{-1, 1}} or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                          \"pass pos_label explicitly.\".format(\n\u001b[0;32m--> 567\u001b[0;31m                              classes_repr=classes_repr))\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y_true takes value in {'animal', 'human'} and pos_label is not specified: either make y_true take value in {0, 1} or {-1, 1} or pass pos_label explicitly."
     ]
    }
   ],
   "source": [
    "run_ensemble(model_paths, test_path, final_test_path, 'lr_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}