{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.10 64-bit ('pytorch_latest_p36': conda)",
   "display_name": "Python 3.6.10 64-bit ('pytorch_latest_p36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4c1e195df8d07db5ee7a78f454b46c3f2e14214bf8c9489d2db5cf8f372ff2ed"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "\n",
    "PATH_ROOT = \"\"\n",
    "PATH_DATA = \"\"\n",
    "\n",
    "creds_path_ar = [\"../../credentials.ini\", \"credentials.ini\"]\n",
    "\n",
    "for creds_path in creds_path_ar:\n",
    "    if path.exists(creds_path):\n",
    "        config_parser = configparser.ConfigParser()\n",
    "        config_parser.read(creds_path)\n",
    "        PATH_ROOT = config_parser['MAIN'][\"PATH_ROOT\"]\n",
    "        PATH_DATA = config_parser['MAIN'][\"PATH_DATA\"]\n",
    "        WANDB_enable = config_parser['MAIN'][\"WANDB_ENABLE\"] == 'TRUE'\n",
    "        ENV = config_parser['MAIN'][\"ENV\"]\n",
    "\n",
    "# adding cwd to path to avoid \"No module named src.*\" errors\n",
    "sys.path.insert(0, os.path.join(PATH_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from src.data import get_data\n",
    "from src.data.iterable_dataset import Config, DataDict, StreamingDataset, iq_to_spectogram, \\\n",
    "    normalize\n",
    "from src.models import arch_setup, tcn_model3\n",
    "from src.data import get_data\n",
    "from src.visualization import metrics\n",
    "from src.features import specto_feat\n",
    "import wandb\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/ubuntu/sota-mafat-radar\n"
     ]
    }
   ],
   "source": [
    "%cd {PATH_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msota-mafat\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.5<br/>\n                Syncing run <strong style=\"color:#cdcd00\">quiet-lake-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/sota-mafat/sota-mafat-radar\" target=\"_blank\">https://wandb.ai/sota-mafat/sota-mafat-radar</a><br/>\n                Run page: <a href=\"https://wandb.ai/sota-mafat/sota-mafat-radar/runs/1usrfe41\" target=\"_blank\">https://wandb.ai/sota-mafat/sota-mafat-radar/runs/1usrfe41</a><br/>\n                Run data is saved locally in <code>wandb/run-20201014_205014-1usrfe41</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<h1>Run(1usrfe41)</h1><p></p><iframe src=\"https://wandb.ai/sota-mafat/sota-mafat-radar/runs/1usrfe41\" style=\"border:none;width:100%;height:400px\"></iframe>",
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4a65191978>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['sota-mafat/sota-mafat-base/1epmi6lf','sota-mafat/sota-mafat-base/3s0bv1dr']\n",
    "test_path = 'MAFAT RADAR Challenge - FULL Public Test Set V1'\n",
    "final_test_path = 'MAFAT RADAR Challenge - Private Test Set V1'\n"
   ]
  },
  {
   "source": [
    "# Two Parts\n",
    "\n",
    "1. Run off the test data to get the test scores for a given model (to give us some indication of the accuracy)\n",
    "2. With the various models, create a LR using the test data as its own train/val."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### PART 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path: str):\n",
    "    '''\n",
    "    Load Model from Wandb\n",
    "    '''\n",
    "    wandb.restore('data/models/model.pth', run_path=model_path)\n",
    "    return torch.load('data/models/model.pth')\n",
    "\n",
    "\n",
    "def load_testset(test_path: str):\n",
    "    '''\n",
    "    Load Test Data\n",
    "    '''\n",
    "    test_data = pd.DataFrame.from_dict(get_data.load_data(test_path, PATH_DATA), orient='index').transpose()\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def run_predictions(model, test_df, final_submission = False):\n",
    "    '''\n",
    "    Have the predictions ready for submission\n",
    "    '''\n",
    "    test_df['output_array'] = test_df['iq_sweep_burst'].progress_apply(iq_to_spectogram)\n",
    "    test_df['output_array'] = test_df.progress_apply(lambda row: specto_feat.max_value_on_doppler(row['output_array'], row['doppler_burst']), axis=1)\n",
    "    test_df['output_array'] = test_df['output_array'].progress_apply(normalize)\n",
    "    test_x = torch.from_numpy(np.stack(test_df['output_array'].tolist(), axis=0).astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu:0')\n",
    "\n",
    "    # Creating DataFrame with the probability prediction for each segment\n",
    "    submission = pd.DataFrame()\n",
    "    submission['segment_id'] = test_df['segment_id']\n",
    "    submission['prediction'] = model(test_x.to(device)).detach().cpu().numpy()\n",
    "    if not final_submission:\n",
    "        submission['label'] = test_df['target_type']\n",
    "    return submission\n",
    "\n",
    "def check_model_auc(model_path: str, test_path: str):\n",
    "    '''\n",
    "    1. Load the Model (using load_model())\n",
    "    2. Load the Test Data (using load_testdata())\n",
    "    3. Return the predictionsauc and acc scores of predictions\n",
    "    '''\n",
    "    model = load_model(model_path)\n",
    "    test_df = load_testset(test_path)\n",
    "    test_df['target_type'].replace({'animal': 0, 'human': 1}, inplace=True)\n",
    "    predictions = run_predictions(model, test_df)\n",
    "    return metrics.model_scores(predictions['label'], predictions['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'sota-mafat/sota-mafat-base/3s0bv1dr'\n",
    "# model = load_model(model_path)\n",
    "# test_path = 'MAFAT RADAR Challenge - FULL Public Test Set V1'\n",
    "# test_dict = load_testset(test_path)\n",
    "# predics = run_predicions(model, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 284/284 [00:00<00:00, 2045.80it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2374.42it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 10783.93it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.675650188604328"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "check_model_auc(model_paths[1],test_path)"
   ]
  },
  {
   "source": [
    "### PART 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LogR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_mean(model_paths: list, test_path, final_submission = False):\n",
    "    preds = []\n",
    "    test_df = load_testset(test_path)\n",
    "    test_df['target_type'].replace({'animal': 0, 'human': 1}, inplace=True)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    pred = df.mean(axis=1)\n",
    "    labels = test_df['target_type']\n",
    "    return metrics.model_scores(labels, pred), final_submission\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(model_paths: list, test_path, final_submission = False):\n",
    "    preds = []\n",
    "    scores = []\n",
    "    test_df = load_testset(test_path)\n",
    "    labels = test_df['target_type']\n",
    "    test_df['target_type'].replace({'animal': 0, 'human': 1}, inplace=True)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "        scores.append(metrics.model_scores(labels,pred['prediction']))\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    scores = np.array(scores)\n",
    "    scores = scores / np.sum(scores)\n",
    "    weighted_mean = (scores*df).mean(axis=1)\n",
    "    return metrics.model_scores(labels, weighted_mean), scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(model_paths: list, test_path, final_submission=False):\n",
    "    preds = []\n",
    "    col_names = range(len(model_paths))\n",
    "    test_df = load_testset(test_path)\n",
    "    test_df['target_type'].replace({'animal': 0, 'human': 1}, inplace=True)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "    df = pd.concat(preds, axis=1)\n",
    "    df.columns = col_names \n",
    "    labels = test_df['target_type']\n",
    "    X_train, X_test,y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=43)\n",
    "    clf = LogR().fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred = (y_pred + 1) / 2\n",
    "    return metrics.model_scores(y_test, y_pred), clf.coef_, clf.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 284/284 [00:00<00:00, 2138.82it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3183.99it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 13386.18it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2151.88it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3190.09it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 12823.03it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.6841133004926109, array([[0.77907647, 0.77907647]]), array([-0.7626038]))"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "lr_model(model_paths, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble(model_paths, old_test_path, final_test_path, ensemble_method, final_submission= True):\n",
    "    preds = []\n",
    "    test_df = load_testset(final_test_path)\n",
    "    for model_path in model_paths:\n",
    "        model = load_model(model_path)\n",
    "        pred = run_predictions(model, test_df, final_submission)\n",
    "        preds.append(pred['prediction'])\n",
    "    \n",
    "    df = pd.concat(preds, axis=1)\n",
    "\n",
    "    if ensemble_method == \"weighted_mean\":\n",
    "        _, scores = weighted_mean(model_paths, old_test_path, final_submission)\n",
    "        prediction = (scores*df).mean(axis=1)\n",
    "    elif ensemble_method == \"lr_model\":\n",
    "        _, coef, bias = lr_model(model_paths, old_test_path, True)\n",
    "        prediction = (coef*df).mean(axis=1) + bias\n",
    "        prediction = (prediction + 1) / 2\n",
    "    else:\n",
    "        prediction = df.mean(axis=1)\n",
    "        \n",
    "    submission = pd.DataFrame()\n",
    "    test_df = pd.DataFrame.from_dict(get_data.load_data(final_test_path, PATH_DATA), orient='index').transpose()\n",
    "    submission['segment_id'] = test_df['segment_id']\n",
    "    submission['prediction'] = prediction\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 284/284 [00:00<00:00, 2134.64it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3255.03it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 13635.02it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2164.85it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3245.63it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 13887.45it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2137.06it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3063.87it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 14223.92it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2139.92it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3158.19it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 14299.74it/s]\n",
      "(284,)\n",
      "100%|██████████| 284/284 [00:00<00:00, 1889.70it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3330.53it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 14307.12it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2161.62it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3339.04it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 8611.72it/s]\n",
      "lr_model\n",
      "100%|██████████| 248/248 [00:00<00:00, 2110.36it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 3372.63it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 9210.66it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 2120.92it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 3020.75it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 13688.48it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 2167.04it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3362.30it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 13689.08it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 1946.91it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 3002.01it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 13338.21it/s]\n"
     ]
    }
   ],
   "source": [
    "all_options = ['basic_mean','weighted_mean','lr_model']\n",
    "best_score = 0\n",
    "best_option = None\n",
    "for option in all_options:\n",
    "    score = eval(option)(model_paths, test_path)\n",
    "    if score[0] > best_score:\n",
    "        best_score = score[0]\n",
    "        best_option = option\n",
    "\n",
    "print(best_option)\n",
    "\n",
    "submission = run_ensemble(model_paths, test_path, final_test_path, option)"
   ]
  }
 ]
}