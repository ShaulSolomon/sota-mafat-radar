{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitvenvmafatvenv300c218a63dc4f9ea0765cbe246bbed9",
   "display_name": "Python 3.6.9 64-bit ('venv_mafat': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "from boto import s3\n",
    "import boto3\n",
    "from boto import boto\n",
    "import re\n",
    "import ssl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from configparser import ConfigParser\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path\n",
    "from os import path\n",
    "from importlib import reload\n",
    "import wandb\n",
    "\n",
    "\n",
    "creds_path_ar = [\"../credentials.ini\",\"credentials.colab.ini\"]\n",
    "\n",
    "for creds_path in creds_path_ar:\n",
    "    if path.exists(creds_path):\n",
    "        config_parser = configparser.ConfigParser()\n",
    "        config_parser.read(creds_path)\n",
    "        AWS_ACCESS_KEY = config_parser['MAIN'][\"AWS_ACCESS_KEY\"]\n",
    "        AWS_ACCESS_SECRET_KEY = config_parser['MAIN'][\"AWS_ACCESS_SECRET_KEY\"]\n",
    "        PATH_ROOT = config_parser['MAIN']['PATH_ROOT']\n",
    "        PATH_DATA = config_parser['MAIN']['PATH_DATA']\n",
    "        break\n",
    "BUCKET='sota-mafat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_creds() -> dict:\n",
    "    \"\"\"\n",
    "    obtain the creds for s3\n",
    "    returns dict\n",
    "    \"\"\"\n",
    "    return s3_dict\n",
    "\n",
    "\n",
    "def list_files_in_bucket(AWS_ACCESS_KEY=AWS_ACCESS_KEY, AWS_ACCESS_SECRET_KEY=AWS_ACCESS_SECRET_KEY, bucket=BUCKET):\n",
    "    \"\"\"\n",
    "    returns a list of file in a bucket\n",
    "    \"\"\"\n",
    "    conn = S3Connection(AWS_ACCESS_KEY, AWS_ACCESS_SECRET_KEY)\n",
    "    conn.auth_region_name = 'us-east-1.amazonaws.com'\n",
    "    mybucket = conn.get_bucket(bucket)\n",
    "    return [i for i in mybucket.list()]\n",
    "\n",
    "\n",
    "def get_dataframe_from_s3(AWS_ACCESS_KEY=AWS_ACCESS_KEY, AWS_ACCESS_SECRET_KEY=AWS_ACCESS_SECRET_KEY, bucket=BUCKET, file=\"data.csv\", type=\"csv\"):\n",
    "    conn = S3Connection(AWS_ACCESS_KEY, AWS_ACCESS_SECRET_KEY)\n",
    "    conn.auth_region_name = 'us-east-1.amazonaws.com'\n",
    "    mybucket = conn.get_bucket(bucket)\n",
    "\n",
    "    # Retrieve Data\n",
    "    key = mybucket.get_key(file)\n",
    "    key.get_contents_to_filename(PATH_ROOT + '/data/' + file)\n",
    "    if type==\"json\":\n",
    "        df = pd.read_json(PATH_ROOT + '/data/' + file, orient=\"records\")\n",
    "    elif type==\"csv\":\n",
    "        df = pd.read_csv(PATH_ROOT + '/data/' + file)\n",
    "    elif type == \"pkl\":\n",
    "        with open(PATH_ROOT + '/data/' + file, 'rb') as data:\n",
    "            df = pickle.load(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_to_s3(file, key, AWS_ACCESS_KEY=AWS_ACCESS_KEY, AWS_ACCESS_SECRET_KEY=AWS_ACCESS_SECRET_KEY, bucket=BUCKET,  callback=None, md5=None, reduced_redundancy=False, content_type=None):\n",
    "    \"\"\"\n",
    "    Uploads the given file to the AWS S3\n",
    "    bucket and key specified.\n",
    "    \n",
    "    callback is a function of the form:\n",
    "    \n",
    "    def callback(complete, total)\n",
    "    \n",
    "    The callback should accept two integer parameters,\n",
    "    the first representing the number of bytes that\n",
    "    have been successfully transmitted to S3 and the\n",
    "    second representing the size of the to be transmitted\n",
    "    object.\n",
    "    \n",
    "    Returns boolean indicating success/failure of upload.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        size = os.fstat(file.fileno()).st_size\n",
    "    except:\n",
    "        # Not all file objects implement fileno(),\n",
    "        # so we fall back on this\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        size = file.tell()\n",
    "\n",
    "    conn = boto.connect_s3(AWS_ACCESS_KEY, AWS_ACCESS_SECRET_KEY)\n",
    "    bucket = conn.get_bucket(bucket, validate=True)\n",
    "    k = Key(bucket)\n",
    "    k.key = key\n",
    "    if content_type:\n",
    "        k.set_metadata('Content-Type', content_type)\n",
    "    sent = k.set_contents_from_file(\n",
    "        file, cb=callback, md5=md5, reduced_redundancy=reduced_redundancy, rewind=True)\n",
    "\n",
    "    # Rewind for later use\n",
    "    file.seek(0)\n",
    "\n",
    "    if sent == size:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def solve_mac_issue():\n",
    "    \"\"\"\n",
    "    code used to solve some issue with ssl in mac\n",
    "    \"\"\"\n",
    "    if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "            getattr(ssl, '_create_unverified_context', None)):\n",
    "            ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe_from_s3(AWS_ACCESS_KEY,AWS_ACCESS_SECRET_KEY,BUCKET,\"MAFAT RADAR Challenge - Public Test Set V1.pkl\",type=\"pkl\")"
   ]
  }
 ]
}